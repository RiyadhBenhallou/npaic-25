{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiyadhBenhallou/npaic-25/blob/main/brain_tumor_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VON44XZfYz9I"
      },
      "source": [
        "# Brain Tumor Segmentation Competition\n",
        "\n",
        "This notebook contains the complete pipeline for the Brain Tumor Segmentation competition.\n",
        "\n",
        "**Steps:**\n",
        "1. Install Dependencies\n",
        "2. Setup Config & Utils\n",
        "3. Define Dataset & Model\n",
        "4. Train Model\n",
        "5. Run Inference on Public Test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yy5TxOBWY5Ns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de77463-65f7-4963-e1a0-c2325553faf4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uKhlaXV3Yz9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0782c7b-be69-4e83-f6aa-e690325f4e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (11.3.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.7.0)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (1.0.22)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.24.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.3)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.12.3)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations) (4.12.0.88)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (4.4.0)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.11.12)\n",
            "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: segmentation-models-pytorch\n",
            "Successfully installed segmentation-models-pytorch-0.5.0\n"
          ]
        }
      ],
      "source": [
        "# 1. Install Dependencies\n",
        "!pip install segmentation-models-pytorch albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVZ69ChCYz9M"
      },
      "outputs": [],
      "source": [
        "# 2. Unzip Data (Adjust paths if you uploaded differently)\n",
        "# !unzip -q train.zip\n",
        "# !unzip -q publictest.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fTEX0v0TYz9M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset as BaseDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import segmentation_models_pytorch as smp\n",
        "import albumentations as albu\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a0odNiz8Yz9N"
      },
      "outputs": [],
      "source": [
        "# 3. Configuration\n",
        "DATA_DIR = '/content/drive/MyDrive/BBA AI Challenge 2025/train'\n",
        "IMAGES_DIR = os.path.join(DATA_DIR, 'images')\n",
        "MASKS_DIR = os.path.join(DATA_DIR, 'masks')\n",
        "TEST_IMGS_DIR = './content/drive/MyDrive/BBA AI Challenge 2025/public_test/images'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/BBA AI Challenge 2025/public_test/masks'\n",
        "\n",
        "ENCODER = 'resnet34'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "CLASSES = ['tumor']\n",
        "ACTIVATION = 'sigmoid'\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 20\n",
        "LR = 0.0001\n",
        "NUM_WORKERS = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wQxGjYWgYz9O"
      },
      "outputs": [],
      "source": [
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "        albu.HorizontalFlip(p=0.5),\n",
        "        albu.VerticalFlip(p=0.5),\n",
        "        albu.ShiftScaleRotate(scale_limit=0.1, rotate_limit=15, shift_limit=0.1, p=0.5, border_mode=0),\n",
        "        albu.PadIfNeeded(min_height=512, min_width=512, always_apply=True, border_mode=0),\n",
        "        albu.RandomCrop(height=512, width=512, always_apply=True),\n",
        "        albu.GaussNoise(p=0.2),\n",
        "        albu.Perspective(p=0.5),\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.CLAHE(p=1),\n",
        "                albu.RandomBrightnessContrast(p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.Sharpen(p=1),\n",
        "                albu.Blur(blur_limit=3, p=1),\n",
        "                albu.MotionBlur(blur_limit=3, p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.RandomBrightnessContrast(p=1),\n",
        "                albu.HueSaturationValue(p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "    ]\n",
        "    return albu.Compose(train_transform)\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    # Standardize size\n",
        "    test_transform = [\n",
        "        albu.PadIfNeeded(512, 512)\n",
        "    ]\n",
        "    return albu.Compose(test_transform)\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    _transform = [\n",
        "        albu.Lambda(image=preprocessing_fn),\n",
        "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    return albu.Compose(_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "e4v9LVViYz9P"
      },
      "outputs": [],
      "source": [
        "# 5. Dataset Class\n",
        "\n",
        "class BrainTumorDataset(BaseDataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            images_dir,\n",
        "            masks_dir,\n",
        "            image_ids=None,\n",
        "            augmentation=None,\n",
        "            preprocessing=None,\n",
        "    ):\n",
        "        self.ids = image_ids if image_ids is not None else os.listdir(images_dir)\n",
        "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
        "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
        "\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        image = cv2.imread(self.images_fps[i])\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        mask = cv2.imread(self.masks_fps[i], 0)\n",
        "        # Binary mask > 127 is tumor\n",
        "        mask = np.where(mask > 127, 1.0, 0.0)\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BElhilGXYz9Q"
      },
      "outputs": [],
      "source": [
        "# 6. Model & Loop Helpers\n",
        "\n",
        "def get_custom_model():\n",
        "    model = smp.Unet(\n",
        "        encoder_name=ENCODER,\n",
        "        encoder_weights=ENCODER_WEIGHTS,\n",
        "        classes=1,\n",
        "        activation=ACTIVATION,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def train_epoch(model, loss, metrics, optimizer, loader, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_iou = 0\n",
        "    epoch_f1 = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    for i, (images, masks) in enumerate(loader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        prediction = model(images)\n",
        "        loss_value = loss(prediction, masks)\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss_value.item()\n",
        "\n",
        "        tp, fp, fn, tn = smp.metrics.get_stats(prediction, masks.long(), mode='binary', threshold=0.5)\n",
        "        iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
        "        f1 = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\")\n",
        "        acc = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\")\n",
        "\n",
        "        epoch_iou += iou\n",
        "        epoch_f1 += f1\n",
        "        epoch_acc += acc\n",
        "\n",
        "    return epoch_loss / len(loader), epoch_iou / len(loader), epoch_f1 / len(loader), epoch_acc / len(loader)\n",
        "\n",
        "def valid_epoch(model, loss, metrics, loader, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_iou = 0\n",
        "    epoch_f1 = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            prediction = model(images)\n",
        "            loss_value = loss(prediction, masks)\n",
        "\n",
        "            epoch_loss += loss_value.item()\n",
        "\n",
        "            tp, fp, fn, tn = smp.metrics.get_stats(prediction, masks.long(), mode='binary', threshold=0.5)\n",
        "            iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
        "            f1 = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\")\n",
        "            acc = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\")\n",
        "\n",
        "            epoch_iou += iou\n",
        "            epoch_f1 += f1\n",
        "            epoch_acc += acc\n",
        "\n",
        "    return epoch_loss / len(loader), epoch_iou / len(loader), epoch_f1 / len(loader), epoch_acc / len(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcBzqx8SYz9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2148e1a2-b232-4ff9-fcd7-29a93358f5e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Train size: 1960, Valid size: 491\n",
            "Starting Training...\n",
            "\n",
            "Epoch: 1/20 (5.0%)\n",
            "Train - Loss: 0.9513, IoU: 0.0719, F1: 0.1323, Acc: 0.6934\n",
            "Valid - Loss: 0.9596, IoU: 0.1956, F1: 0.3239, Acc: 0.9366\n",
            "Model saved!\n",
            "\n",
            "Epoch: 2/20 (10.0%)\n",
            "Train - Loss: 0.9479, IoU: 0.1416, F1: 0.2451, Acc: 0.8792\n",
            "Valid - Loss: 0.9579, IoU: 0.2008, F1: 0.3301, Acc: 0.9351\n",
            "Model saved!\n",
            "\n",
            "Epoch: 3/20 (15.0%)\n",
            "Train - Loss: 0.9457, IoU: 0.1829, F1: 0.3050, Acc: 0.9098\n",
            "Valid - Loss: 0.9574, IoU: 0.2262, F1: 0.3638, Acc: 0.9447\n",
            "Model saved!\n",
            "\n",
            "Epoch: 4/20 (20.0%)\n",
            "Train - Loss: 0.9446, IoU: 0.1856, F1: 0.3081, Acc: 0.9088\n",
            "Valid - Loss: 0.9566, IoU: 0.2548, F1: 0.4017, Acc: 0.9533\n",
            "Model saved!\n",
            "\n",
            "Epoch: 5/20 (25.0%)\n",
            "Train - Loss: 0.9444, IoU: 0.2071, F1: 0.3376, Acc: 0.9219\n",
            "Valid - Loss: 0.9565, IoU: 0.2554, F1: 0.4020, Acc: 0.9528\n",
            "Model saved!\n",
            "\n",
            "Epoch: 6/20 (30.0%)\n",
            "Train - Loss: 0.9442, IoU: 0.2288, F1: 0.3654, Acc: 0.9299\n",
            "Valid - Loss: 0.9564, IoU: 0.2900, F1: 0.4431, Acc: 0.9598\n",
            "Model saved!\n",
            "\n",
            "Epoch: 7/20 (35.0%)\n",
            "Train - Loss: 0.9437, IoU: 0.2399, F1: 0.3795, Acc: 0.9339\n",
            "Valid - Loss: 0.9561, IoU: 0.3075, F1: 0.4635, Acc: 0.9628\n",
            "Model saved!\n",
            "\n",
            "Epoch: 8/20 (40.0%)\n",
            "Train - Loss: 0.9423, IoU: 0.2603, F1: 0.4054, Acc: 0.9396\n",
            "Valid - Loss: 0.9560, IoU: 0.3445, F1: 0.5062, Acc: 0.9692\n",
            "Model saved!\n",
            "\n",
            "Epoch: 9/20 (45.0%)\n",
            "Train - Loss: 0.9424, IoU: 0.3025, F1: 0.4567, Acc: 0.9515\n",
            "Valid - Loss: 0.9559, IoU: 0.3274, F1: 0.4857, Acc: 0.9660\n",
            "\n",
            "Epoch: 10/20 (50.0%)\n",
            "Train - Loss: 0.9418, IoU: 0.2933, F1: 0.4450, Acc: 0.9483\n",
            "Valid - Loss: 0.9559, IoU: 0.3748, F1: 0.5380, Acc: 0.9727\n",
            "Model saved!\n",
            "\n",
            "Epoch: 11/20 (55.0%)\n",
            "Train - Loss: 0.9437, IoU: 0.2247, F1: 0.3602, Acc: 0.9280\n",
            "Valid - Loss: 0.9563, IoU: 0.2634, F1: 0.4106, Acc: 0.9545\n",
            "\n",
            "Epoch: 12/20 (60.0%)\n",
            "Train - Loss: 0.9443, IoU: 0.2383, F1: 0.3779, Acc: 0.9345\n",
            "Valid - Loss: 0.9563, IoU: 0.2786, F1: 0.4293, Acc: 0.9583\n",
            "\n",
            "Epoch: 13/20 (65.0%)\n",
            "Train - Loss: 0.9431, IoU: 0.2551, F1: 0.3986, Acc: 0.9385\n",
            "Valid - Loss: 0.9560, IoU: 0.3119, F1: 0.4684, Acc: 0.9642\n",
            "\n",
            "Epoch: 14/20 (70.0%)\n",
            "Train - Loss: 0.9441, IoU: 0.2610, F1: 0.4066, Acc: 0.9411\n",
            "Valid - Loss: 0.9559, IoU: 0.3964, F1: 0.5617, Acc: 0.9759\n",
            "Model saved!\n",
            "\n",
            "Epoch: 15/20 (75.0%)\n",
            "Train - Loss: 0.9426, IoU: 0.2804, F1: 0.4302, Acc: 0.9461\n",
            "Valid - Loss: 0.9558, IoU: 0.3709, F1: 0.5334, Acc: 0.9726\n",
            "\n",
            "Epoch: 16/20 (80.0%)\n",
            "Train - Loss: 0.9428, IoU: 0.2867, F1: 0.4368, Acc: 0.9475\n",
            "Valid - Loss: 0.9560, IoU: 0.3000, F1: 0.4517, Acc: 0.9600\n",
            "\n",
            "Epoch: 17/20 (85.0%)\n",
            "Train - Loss: 0.9420, IoU: 0.2822, F1: 0.4327, Acc: 0.9460\n",
            "Valid - Loss: 0.9558, IoU: 0.3521, F1: 0.5082, Acc: 0.9682\n",
            "\n",
            "Epoch: 18/20 (90.0%)\n"
          ]
        }
      ],
      "source": [
        "# 7. Train\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
        "\n",
        "all_files = os.listdir(IMAGES_DIR)\n",
        "train_ids, valid_ids = train_test_split(all_files, test_size=0.2, random_state=42)\n",
        "print(f\"Train size: {len(train_ids)}, Valid size: {len(valid_ids)}\")\n",
        "\n",
        "train_dataset = BrainTumorDataset(\n",
        "    IMAGES_DIR,\n",
        "    MASKS_DIR,\n",
        "    image_ids=train_ids,\n",
        "    augmentation=get_training_augmentation(),\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        ")\n",
        "\n",
        "valid_dataset = BrainTumorDataset(\n",
        "    IMAGES_DIR,\n",
        "    MASKS_DIR,\n",
        "    image_ids=valid_ids,\n",
        "    augmentation=get_validation_augmentation(),\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "model = get_custom_model()\n",
        "model.to(DEVICE)\n",
        "\n",
        "loss = smp.losses.DiceLoss(mode='binary')\n",
        "optimizer = torch.optim.Adam([dict(params=model.parameters(), lr=LR)])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-5, last_epoch=-1)\n",
        "\n",
        "max_score = 0\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "for i in range(EPOCHS):\n",
        "    print(f'\\nEpoch: {i+1}/{EPOCHS} ({(i+1)/EPOCHS*100:.1f}%)')\n",
        "    train_logs = train_epoch(model, loss, [], optimizer, train_loader, DEVICE)\n",
        "    valid_logs = valid_epoch(model, loss, [], valid_loader, DEVICE)\n",
        "\n",
        "    print(f\"Train - Loss: {train_logs[0]:.4f}, IoU: {train_logs[1]:.4f}, F1: {train_logs[2]:.4f}, Acc: {train_logs[3]:.4f}\")\n",
        "    print(f\"Valid - Loss: {valid_logs[0]:.4f}, IoU: {valid_logs[1]:.4f}, F1: {valid_logs[2]:.4f}, Acc: {valid_logs[3]:.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if valid_logs[1] > max_score:\n",
        "        max_score = valid_logs[1]\n",
        "        torch.save(model, './best_model.pth')\n",
        "        print('Model saved!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDYxL0hVYz9S"
      },
      "outputs": [],
      "source": [
        "# 8. Inference\n",
        "class TestDataset(Dataset):  # Re-define to capture notebook scope (preprocessing_fn)\n",
        "    def __init__(self, images_dir):\n",
        "        self.ids = os.listdir(images_dir)\n",
        "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
        "\n",
        "        preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
        "        self.preprocessing = self.get_preprocessing(preprocessing_fn)\n",
        "\n",
        "    def get_preprocessing(self, preprocessing_fn):\n",
        "        _transform = [\n",
        "            albu.PadIfNeeded(512, 512),\n",
        "            albu.Lambda(image=preprocessing_fn),\n",
        "            albu.Lambda(image=to_tensor),\n",
        "        ]\n",
        "        return albu.Compose(_transform)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        image = cv2.imread(self.images_fps[i])\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image)\n",
        "            image = sample['image']\n",
        "        return image, self.ids[i]\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "print(\"Loading Best Model...\")\n",
        "best_model = torch.load('./best_model.pth', map_location=DEVICE)\n",
        "best_model.eval()\n",
        "\n",
        "test_dataset = TestDataset(TEST_IMGS_DIR)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print(\"Generating Predictions...\")\n",
        "with torch.no_grad():\n",
        "    for images, ids in test_loader:\n",
        "        images = images.to(DEVICE)\n",
        "\n",
        "        # TTA Horizontal Flip\n",
        "        pr_mask = best_model(images)\n",
        "        images_flip = torch.flip(images, dims=[3])\n",
        "        pr_mask_flip = best_model(images_flip)\n",
        "        pr_mask_flip = torch.flip(pr_mask_flip, dims=[3])\n",
        "        pr_mask = (pr_mask + pr_mask_flip) / 2.0\n",
        "\n",
        "        pr_mask = (pr_mask > 0.5).float().cpu().numpy().squeeze()\n",
        "\n",
        "        for i, image_id in enumerate(ids):\n",
        "            mask = pr_mask[i]\n",
        "            mask = (mask * 255).astype(np.uint8)\n",
        "            # Resize if necessary (if input was padding, we might need to crop, but 512 is target)\n",
        "            if mask.shape != (512, 512):\n",
        "                 mask = cv2.resize(mask, (512, 512), interpolation=cv2.INTER_NEAREST)\n",
        "            cv2.imwrite(os.path.join(OUTPUT_DIR, image_id), mask)\n",
        "\n",
        "print(\"Done!\")\n",
        "\n",
        "!zip -r predictions.zip predictions"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}